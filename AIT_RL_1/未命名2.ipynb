{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from contextlib import closing\n",
    "\n",
    "import numpy as np\n",
    "from six import StringIO, b\n",
    "\n",
    "from gym import utils\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "MAPS = {\n",
    "    \"theAlley\": [\n",
    "        \"S...H...H...G\"\n",
    "    ],\n",
    "    \"walkInThePark\": [\n",
    "        \"S.......\",\n",
    "        \".....H..\",\n",
    "        \"........\",\n",
    "        \"......H.\",\n",
    "        \"........\",\n",
    "        \"...H...G\"\n",
    "    ],\n",
    "    \"1Dtest\": [\n",
    "\n",
    "    ],\n",
    "    \"4x4\": [\n",
    "        \"S...\",\n",
    "        \".H.H\",\n",
    "        \"...H\",\n",
    "        \"H..G\"\n",
    "    ],\n",
    "    \"8x8\": [\n",
    "        \"S.......\",\n",
    "        \"........\",\n",
    "        \"...H....\",\n",
    "        \".....H..\",\n",
    "        \"...H....\",\n",
    "        \".HH...H.\",\n",
    "        \".H..H.H.\",\n",
    "        \"...H...G\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "POTHOLE_PROB = 0.2\n",
    "BROKEN_LEG_PENALTY = -10\n",
    "SLEEP_DEPRIVATION_PENALTY = -0.0\n",
    "REWARD = 10\n",
    "\n",
    "def generate_random_map(size=8, p=0.8):\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    :param size: size of each side of the grid\n",
    "    :param p: probability that a tile is frozen\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(res):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((0,0))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r,c) in discovered:\n",
    "                discovered.add((r,c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == 'G':\n",
    "                        return True\n",
    "                    if (res[r_new][c_new] not in '#H'):\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice(['.', 'H'], (size, size), p=[p, 1-p])\n",
    "        res[0][0] = 'S'\n",
    "        res[-1][-1] = 'G'\n",
    "        valid = is_valid(res)\n",
    "    return [\"\".join(x) for x in res]\n",
    "\n",
    "\n",
    "class DrunkenWalkEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    A simple grid environment, completely based on the code of 'FrozenLake', credits to \n",
    "    the original authors.\n",
    "\n",
    "    You're finding your way home (G) after a great party which was happening at (S).\n",
    "    Unfortunately, due to recreational intoxication you find yourself only moving into \n",
    "    the intended direction 80% of the time, and perpendicular to that the other 20%.\n",
    "\n",
    "    To make matters worse, the local community has been cutting the budgets for pavement\n",
    "    maintenance, which means that the way to home is full of potholes, which are very likely\n",
    "    to make you trip. If you fall, you are obviously magically transported back to the party, \n",
    "    without getting some of that hard-earned sleep.\n",
    "\n",
    "        S...\n",
    "        .H.H\n",
    "        ...H\n",
    "        H..G\n",
    "    H : pothole, you have a POTHOLE_PROB chance of tripping\n",
    "    G : goal, time for bed\n",
    "\n",
    "    The episode ends when you reach the goal or trip.\n",
    "    You receive a reward of +10 if you reach the goal, \n",
    "    but get a SLEEP_DEPRIVATION_PENALTY and otherwise.  剥夺睡眠\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}  # the meaning of ansi\n",
    "\n",
    "    def __init__(self, desc=None, map_name=\"4x4\",is_slippery=True):\n",
    "        \"\"\" This generates a map and sets all transition probabilities.\n",
    "\n",
    "            (by passing constructed nS, nA, P, isd to DiscreteEnv)\n",
    "        \"\"\"\n",
    "        if desc is None and map_name is None:\n",
    "            desc = generate_random_map()\n",
    "        elif desc is None:\n",
    "            desc = MAPS[map_name]\n",
    "\n",
    "        self.desc = desc = np.asarray(desc,dtype='c')\n",
    "        self.nrow, self.ncol = nrow, ncol = desc.shape  # start 6*8\n",
    "        self.reward_range = (0, 1)\n",
    "\n",
    "        nA = 4\n",
    "        nS = nrow * ncol\n",
    "        # compare desc with S, find the big S position\n",
    "        isd = np.array(desc == b'S').astype('float64').ravel() # ravel: transfer n-D to 1-D \n",
    "#         print(isd)\n",
    "# [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    "#  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "# ---episode 0---\n",
    "        isd /= isd.sum()\n",
    "\n",
    "        # We need to pass 'P' to DiscreteEnv:\n",
    "        # P dictionary dict of dicts of lists, where\n",
    "        # P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "        P = {s : {a : [] for a in range(nA)} for s in range(nS)}  # how can P represent so many params\n",
    "\n",
    "        def to_s(row, col):\n",
    "            return row*ncol + col\n",
    "\n",
    "        #def inc(row, col, a):\n",
    "        def intended_destination(row, col, a):\n",
    "            if a == LEFT:\n",
    "                col = max(col-1,0)\n",
    "            elif a == DOWN:\n",
    "                row = min(row+1,nrow-1)\n",
    "            elif a == RIGHT:\n",
    "                col = min(col+1,ncol-1)\n",
    "            elif a == UP:\n",
    "                row = max(row-1,0)\n",
    "            return (row, col)\n",
    "\n",
    "        def construct_transition_for_intended(row, col, a, prob, li):\n",
    "            \"\"\" this constructs a transition to the \"intended_destination(row, col, a)\"\n",
    "                and adds it to the transition list (which could be for a different action b).\n",
    "\n",
    "            \"\"\"\n",
    "            newrow, newcol = intended_destination(row, col, a)  # the next step position\n",
    "            newstate = to_s(newrow, newcol)   # how to understand the new state is a integer varaible\n",
    "            newletter = desc[newrow, newcol]  # record the new letter\n",
    "            done = bytes(newletter) in b'G'   # when home is done\n",
    "            rew = REWARD if newletter == b'G' else SLEEP_DEPRIVATION_PENALTY\n",
    "            li.append( (prob, newstate, rew, done) )\n",
    "\n",
    "\n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                # specify transitions for s=(row, col)\n",
    "                s = to_s(row, col)\n",
    "                letter = desc[row, col]\n",
    "                for a in range(4):\n",
    "                    # specify transitions for action a\n",
    "                    li = P[s][a] # s is states, a is action\n",
    "                    if letter in b'G':\n",
    "                        #when we are at goal, we reset with prob. 1\n",
    "                        li.append((1.0, s, 42, True))   # prob newstate reward done\n",
    "                        #really, this should not be happening, since we get done\n",
    "                        #when transitioning TO the g\n",
    "                        # with prob. 0.8 we move as intended:\n",
    "                        construct_transition_for_intended(row, col, a, 0.8, li)\n",
    "                        # but with prob. 0.1 we move sideways to intended:\n",
    "                        for b in [(a-1)%4, (a+1)%4]: # 其他的  0.1\n",
    "                            construct_transition_for_intended(row, col, b, 0.1, li)\n",
    "\n",
    "        super(DrunkenWalkEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "    def action_to_string(self, action_index):\n",
    "        s =\"{}\".format([\"Left\",\"Down\",\"Right\",\"Up\"][action_index])\n",
    "        return s\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        row, col = self.s // self.ncol, self.s % self.ncol\n",
    "        desc = self.desc.tolist()\n",
    "        desc = [[c.decode('utf-8') for c in line] for line in desc]\n",
    "        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\" (last action was '{action}')\\n\".format( action=self.action_to_string(self.lastaction) ) )\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\".join(''.join(line) for line in desc)+\"\\n\")\n",
    "\n",
    "        if mode != 'human':\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    价值迭代算法\n",
    "    参数:\n",
    "        env: OpenAI environment。env.P代表环境的转移概率p(s',r|s,a)\n",
    "        theta: Stopping threshold\n",
    "        discount_factor: 打折因子gamma\n",
    "        \n",
    "    返回值:\n",
    "        二元组(policy, V)代表最优策略和最优价值函数\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        给定一个状态，根据Value Iteration公式计算新的V(s)\n",
    "        参数:\n",
    "            state: 状态(int)\n",
    "            V: 当前的V(s)，长度是env.nS\n",
    "        \n",
    "        返回:\n",
    "            返回新的V(s)\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        \n",
    "        for s in range(env.nS):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function\n",
    "            V[s] = best_action_value        \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'env'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1bfbc1a0fdbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridworld\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridworldEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# env = DrunkenWalkEnv(map_name=\"walkInThePark\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridworldEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'env'"
     ]
    }
   ],
   "source": [
    "from env.gridworld import GridworldEnv\n",
    "# env = DrunkenWalkEnv(map_name=\"walkInThePark\")\n",
    "env = GridworldEnv()\n",
    "env.reset()\n",
    "\n",
    "policy, v = value_iteration(env)\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "# print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "# print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
