{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG=False\n",
    "#DEBUG=True\n",
    "\n",
    "def debug_q_update(prev_obs,  action, observation, reward, done, predict, gamma, future_val, target, td, new_q ):\n",
    "    #print(\"Transition: %s\" % ( ( prev_obs,  action, observation, reward, done), ) )\n",
    "    if not DEBUG:\n",
    "        return\n",
    "        \n",
    "    print(\"\"\"Q update:\n",
    "        predict = %f\n",
    "        reward =  %f\n",
    "        gamma =   %f\n",
    "        futval =  %f\n",
    "        target =  %f\n",
    "        td =      %f\n",
    "        new_q =   %f\"\"\" % (predict, reward, gamma, future_val, target, td, new_q) )\n",
    "\n",
    "    if reward != 0 or target != 0:\n",
    "        input(\"press enter to continue\")\n",
    "\n",
    "def callersname():\n",
    "    import sys\n",
    "    return sys._getframe(2).f_code.co_name\n",
    "\n",
    "def nyi_warn(obj):\n",
    "    s = \"'%s()' not yet implemented for: '%s'\" % (callersname(), obj.__str__())\n",
    "    print(s)\n",
    "\n",
    "def nyi_exc(obj):\n",
    "    s = \"'%s()' not yet implemented for: '%s'\" % (callersname(), obj.__str__())\n",
    "    raise Exception(s)\n",
    "\n",
    "def override_exc(obj):\n",
    "    s = \"'%s()' not overriden by self: '%s'\" % (callersname(), obj.__str__())\n",
    "    raise Exception(s)\n",
    "\n",
    "\n",
    "def assert_isinstance(v, cl):\n",
    "    if not isinstance(v, cl):\n",
    "        s =  \"'%s' is no instance of %s!\" %(v.__str__(), cl)\n",
    "        raise Exception(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import debug_utils\n",
    "\n",
    "NUM_EPISODES = 500\n",
    "MAX_EPISODE_LENGTH = 30000\n",
    "\n",
    "RMSIZE = 10000  # replay memory size\n",
    "BATCH_SIZE = 256  # size of replay memory batch (= the number of updates for each real step)\n",
    "\n",
    "# consistent with Shiverma's code:\n",
    "DEFAULT_DISCOUNT = 0.99\n",
    "EPSILON = 1\n",
    "LEARNINGRATENET = 0.0001  # QNET\n",
    "\n",
    "\n",
    "# TODO coding exercise 1: implement experience replay\n",
    "class ReplayMemory(object):\n",
    "    # ReplayMemory should store the last \"size\" experiences\n",
    "    # and be able to return a randomly sampled batch of experiences\n",
    "    def __init__(self, size):\n",
    "        pass\n",
    "\n",
    "    # Store experience in memory\n",
    "    def store_experience(self, prev_obs, action, observation, reward, done):\n",
    "        pass\n",
    "\n",
    "    # Randomly sample \"batch_size\" experiences from the memory and return them\n",
    "    def sample_batch(self, batch_size):\n",
    "        pass\n",
    "\n",
    "\n",
    "# DEBUG=True\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "class QNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_a, obs_shape, discount=DEFAULT_DISCOUNT, learning_rate=LEARNINGRATENET):\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.discount = discount\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def init_optimizer(self):\n",
    "        self.loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "        self.optimizer = torch.optim.RMSprop(self.parameters(), lr=self.learning_rate, alpha=0.9)\n",
    "\n",
    "    def obs_to_tensor(self, obs):\n",
    "        \"\"\" QNet uses pytorch, and hence all observations need to be wrapped as tensors \"\"\"\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            if np.isscalar(obs):\n",
    "                obs = [obs]\n",
    "            obs = torch.Tensor(obs)\n",
    "        return obs\n",
    "\n",
    "    def max_Q_value(self, observation, batch_size=1):\n",
    "        observation = self.obs_to_tensor(observation)\n",
    "        Qs = self.forward(observation)  # <- this should feed in the input\n",
    "        # print (\"QNet::max_Q_value: Qs: %s\", Qs)\n",
    "\n",
    "        if batch_size > 1:\n",
    "            v, _ = Qs.max(dim=1)\n",
    "        else:\n",
    "            v = Qs.max()\n",
    "        v = v.detach().numpy()\n",
    "        # print (\"... Vs: %s\" % v)\n",
    "        return v\n",
    "\n",
    "    def argmax_Q_value(self, observation):\n",
    "        \"\"\" observation is a single observation - does not work for batch \"\"\"\n",
    "        observation = self.obs_to_tensor(observation)\n",
    "        t_Qs = self.forward(observation)  # <- this should feed in the input\n",
    "\n",
    "        # instead:\n",
    "        Qs = t_Qs.detach().numpy()\n",
    "        if DEBUG:\n",
    "            print(\"argmax_Q_value: Qs: %s\" % Qs)\n",
    "        m = np.random.choice(np.flatnonzero(Qs == Qs.max()))\n",
    "        return m\n",
    "\n",
    "    def get_Q(self, o, a, batch_size=1):\n",
    "        observation = self.obs_to_tensor(o)\n",
    "        Qs = self.forward(observation)\n",
    "        if batch_size > 1:\n",
    "            q = Qs[range(batch_size), a]\n",
    "        else:\n",
    "            q = Qs[a]\n",
    "\n",
    "        return q\n",
    "\n",
    "    def single_Q_update(self, prev_observation, action, observation, reward, done):\n",
    "        \"\"\" action and observation need to be in the format that QNet was constructed for.\n",
    "            I.e., if observation is a discrete variable (with say N values=states), but QNet\n",
    "            is working on one-hot vectors (of length N), then observation needs to be such a\n",
    "            one-hot vector.\n",
    "\n",
    "            QNet is not responsible for conversion\n",
    "        \"\"\"\n",
    "        t_observation = self.obs_to_tensor(observation)\n",
    "        t_prev_obs = self.obs_to_tensor(prev_observation)\n",
    "\n",
    "        if done:\n",
    "            future_val = 0  # XXX<- needs to be a 0-tensor?\n",
    "        else:\n",
    "            future_val = self.max_Q_value(t_observation)  ##<<- this evaluates the QNet\n",
    "        # We just evaluated the Qnet for the next-stage variables, but of course... the effect of the Qnet\n",
    "        # parameters on the *next-stage* value is ignored by Q-learning.\n",
    "        # So... we need to reset the gradients. (otherwise they accumulate e.g., see;\n",
    "        # https://medium.com/@zhang_yang/how-pytorch-tensors-backward-accumulates-gradient-8d1bf675579b)\n",
    "        self.zero_grad()\n",
    "\n",
    "        t_predict = self.get_Q(t_prev_obs, action)  ##<<- this evaluates the QNet\n",
    "        t_predict.backward()  # computes grad_theta Q(s,a)\n",
    "\n",
    "        # I suppose this could be parallelized when making it torch:\n",
    "        target = reward + self.discount * future_val\n",
    "        td = target - t_predict.detach().numpy()\n",
    "\n",
    "        # now update all the parameters\n",
    "        for param in self.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)  # <--- apply gradient clipping to avoid exploding gradients...\n",
    "            param.data.add_(self.learning_rate * td * param.grad.data)\n",
    "\n",
    "        self.zero_grad()\n",
    "\n",
    "        predict = t_predict.detach().numpy()\n",
    "        new_q = self.get_Q(t_prev_obs, action).detach().numpy()\n",
    "        self.zero_grad()\n",
    "\n",
    "        debug_utils.debug_q_update(prev_observation, action, observation, reward, done, predict, self.discount, future_val,\n",
    "                                   target, td, new_q)\n",
    "\n",
    "    def batch_Q_update(self, obs, actions, next_obs, rewards, dones):\n",
    "\n",
    "        batch_size = len(dones)\n",
    "        v_next_obs = self.max_Q_value(next_obs, batch_size)\n",
    "        not_dones = 1 - dones\n",
    "        fut_values = self.discount * v_next_obs * not_dones\n",
    "        targets = rewards + fut_values\n",
    "\n",
    "        self.zero_grad()\n",
    "        q_pred = self.get_Q(obs, actions, batch_size)\n",
    "        loss = self.loss_fn(q_pred, torch.tensor(targets, dtype=torch.float))\n",
    "        if DEBUG:\n",
    "            print(\"q_pred:    %s\" % q_pred)\n",
    "            print(\"loss:      %s\" % loss.item())\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "class QNet_MLP(QNet):\n",
    "    def __init__(self, num_a, obs_shape, discount=DEFAULT_DISCOUNT, learning_rate=LEARNINGRATENET):\n",
    "        super().__init__(num_a, obs_shape, discount=discount, learning_rate=learning_rate)\n",
    "        self.init_network(obs_shape, num_a)\n",
    "        self.init_optimizer()\n",
    "\n",
    "    def init_network(self, obs_shape, num_a):\n",
    "        num_in = np.prod(obs_shape)\n",
    "        print(\"QNet_MLP initialization: num_in=%s, obs_shape=%s\" % (num_in, obs_shape))\n",
    "        ### MLP\n",
    "        HIDDEN_NODES1 = 150\n",
    "        HIDDEN_NODES2 = 120\n",
    "        self.fc1 = nn.Linear(num_in, HIDDEN_NODES1)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(HIDDEN_NODES1, HIDDEN_NODES2)\n",
    "        self.fc3 = nn.Linear(HIDDEN_NODES2, num_a)  # 4 outputs, the Q-values for the 4 actions\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" This assumes x to be a tensor \"\"\"\n",
    "        debug_utils.assert_isinstance(x, torch.Tensor)\n",
    "        ### MLP:\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        ###----\n",
    "        return x\n",
    "\n",
    "\n",
    "class QLearner(object):\n",
    "    def __init__(self, env, q_function, discount=DEFAULT_DISCOUNT, rm_size=RMSIZE):\n",
    "        self.env = env\n",
    "        self.Q = q_function\n",
    "        self.rm = ReplayMemory(rm_size)  # replay memory stores (a subset of) experience across episode\n",
    "        self.discount = discount\n",
    "\n",
    "        self.epsilon = EPSILON\n",
    "        self.epsilon_min = .01\n",
    "        self.epsilon_decay = .98\n",
    "\n",
    "        self.batch_size = BATCH_SIZE\n",
    "\n",
    "        self.name = \"agent1\"\n",
    "        self.episode = 0\n",
    "        self.cum_r = 0  # cumulative reward in current episode\n",
    "        self.tot_r = 0  # cumulative reward in lifetime\n",
    "        self.stage = 0  # the time step, or 'stage' in this episode\n",
    "        self.tot_stages = 0  # total time steps in lifetime\n",
    "\n",
    "    def reset_episode(self, initial_obs):\n",
    "        self.last_obs = initial_obs\n",
    "        self.tot_r += self.cum_r  # store the reward of the previous episode\n",
    "        self.cum_r = 0  # reset cumulative reward for new episode\n",
    "        self.dis_r = 0  # discounted cum. reward\n",
    "        self.tot_stages += self.stage\n",
    "        self.stage = 0  # reset the time step, or 'stage' in this episode\n",
    "        self.episode += 1\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay # Decay epsilon\n",
    "\n",
    "    def process_experience(self, action, observation, reward, done):\n",
    "        prev_obs = self.last_obs\n",
    "        self.cum_r += reward\n",
    "        self.dis_r += reward * (self.discount ** self.stage)\n",
    "        self.stage += 1\n",
    "        self.Q.single_Q_update(prev_obs, action, observation, reward, done)\n",
    "        self.last_obs = observation\n",
    "\n",
    "        # TODO coding exercise 1: Do a batch update using experience stored in the replay memory\n",
    "        # if self.tot_stages > 10 * self.batch_size:\n",
    "            # sample a batch of batch_size from the replay memory\n",
    "            # and update the network using this batch (batch_Q_update)\n",
    "\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        \"\"\"select an action based in self.last_obs\n",
    "\n",
    "           (In general we might select actions on more general information... i.e., last_obs could\n",
    "            be generalized to last_internal_state )\n",
    "        \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.randint(0, self.env.action_space.n - 1)\n",
    "            if DEBUG:\n",
    "                print(\"select_action_random used\")\n",
    "        else:\n",
    "            action = self.Q.argmax_Q_value(obs)\n",
    "            if DEBUG:\n",
    "                print(\"select_action_greedy used\")\n",
    "\n",
    "        return action\n",
    "\n",
    "    def report(self):\n",
    "        name = self.name\n",
    "        print(\"---\")\n",
    "        print(\"%s: episode: %d\" % (name, self.episode))\n",
    "        print(\"%s: stage:   %d\" % (name, self.stage))\n",
    "        print(\"%s: totals stages:   %d\" % (name, self.tot_stages))\n",
    "        print(\"%s: epsilon: %f\" % (name, self.epsilon))\n",
    "        print(\"%s: cum_r:   %s\" % (name, self.cum_r))\n",
    "        print(\"%s: dis_r:   %s\" % (name, self.dis_r))\n",
    "        mean_r_this_ep = self.cum_r / self.stage if self.stage > 0 else \"undef\"\n",
    "        mean_r = self.tot_r / self.tot_stages if self.tot_stages > 0 else \"undef\"\n",
    "        mean_r_ep = self.tot_r / self.episode if self.episode > 0 else \"undef\"\n",
    "        print(\"%s: mean r in this episode:  %s\" % (name, mean_r_this_ep))\n",
    "        print(\"%s: mean r in lifetime:      %s\" % (name, mean_r))\n",
    "        print(\"%s: mean return per episode:   %s\" % (name, mean_r_ep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from deep_q_learning_skeleton import *\n",
    "\n",
    "# Set to true if you want the agent to take into account the remaining time\n",
    "# (an episode automatically stops after 1000 timesteps)\n",
    "timeHorizon = True\n",
    "\n",
    "def act_loop(env, agent, num_episodes):\n",
    "    for episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        if timeHorizon:\n",
    "            observation = np.append(observation,1)\n",
    "        agent.reset_episode(observation)\n",
    "\n",
    "        print('---episode %d---' % episode)\n",
    "        renderit = False\n",
    "        if episode % 10 == 0:\n",
    "            renderit = True\n",
    "\n",
    "        # for t in range(MAX_EPISODE_LENGTH):\n",
    "        t = 0\n",
    "        while True:\n",
    "            t += 1\n",
    "            if renderit:\n",
    "                env.render()\n",
    "            printing=False\n",
    "            if t % 500 == 499:\n",
    "                printing = True\n",
    "\n",
    "            if printing:\n",
    "                print('---stage %d---' % t)\n",
    "                agent.report()\n",
    "                print(\"obs:\", observation)\n",
    "\n",
    "            action = agent.select_action(observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if timeHorizon:\n",
    "                timeRemaining = (1000 - t) / 1000 # goes from 1 at first timestep to 0 at last timestep\n",
    "                observation = np.append(observation, timeRemaining)\n",
    "            if printing:\n",
    "                print(\"act:\", action)\n",
    "                print(\"reward=%s\" % reward)\n",
    "\n",
    "            agent.process_experience(action, observation, reward, done)\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                env.render()\n",
    "                agent.report()\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # from def_env import env  #<- defines env\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    print(\"action space:\", env.action_space)\n",
    "    print(\"observ space:\", env.observation_space)\n",
    "\n",
    "    num_a = env.action_space.n\n",
    "    shape_o = env.observation_space.shape\n",
    "    if timeHorizon:\n",
    "        shape_o = (9,)\n",
    "\n",
    "    qn = QNet_MLP(num_a, shape_o)\n",
    "\n",
    "    discount = DEFAULT_DISCOUNT\n",
    "\n",
    "    ql = QLearner(env, qn, discount) #<- QNet\n",
    "\n",
    "    # TODO: Coding exercise 2: target network\n",
    "    # target_qn = QNet_MLP(num_a, shape_o)\n",
    "    # target_qn.load_state_dict(qn.state_dict())\n",
    "    # ql = QLearner(env, qn, target_qn, discount)  # <- QNet\n",
    "\n",
    "    act_loop(env, ql, NUM_EPISODES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
